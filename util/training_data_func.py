"""
this module defines all available training functions for generating the training data set
All training function will produce a target value between 0 and 1
"""
from math import exp, sin
from random import uniform
from net_structure import Net_structure
from cost import cost_dict
from node_activity import activation_dict
from numpy import *
from functools import reduce
import pdb

def trainingFunc(funcName):
    """
    first class function: return a closure of the actual training function

    argument:
        funcName        the description of how the training function should behave
    return:
        the actual training function
    """
    if funcName == "Sigmoid":
        def sigmoid(xList):
            """
            simple sigmoid function:
                most suitable for neuron of sigmoid activation
                can be treated as baseline
            """
            xSum = reduce(lambda x1, x2: x1+x2, xList)
            return [ 1 / (1 + exp(-xSum)) ]
        return sigmoid
    elif funcName == "Sin":
        """
        simple sin function
        """
        def sine(xList):
            assert len(xList) >= 1
            sinSum = reduce(lambda x1, x2: x1 + x2, xList)
            return [ sin(sinSum) ]
        return sine
    elif funcName == "AttenSin-abs-x0":
        """
        exponentially attenuating sin, with abs(), and x0 as exponent
        """
        def attenSinAbsX0(xList):
            assert len(xList) >= 1
            expPow = xList[0] * 0.2
            sinSum = reduce(lambda x1, x2: x1 + x2, xList) - xList[0]
            return [ exp(-abs(expPow)) * abs(sin(sinSum)) ]
        return attenSinAbsX0
    elif funcName == "Random":
        def rand(xList):
            return [ uniform(0, 1) ]
        return rand
    elif funcName == "AttenSin-x0":
        """
        no abs(), but exponent is still controlled separately by x0
        """
        def attenSinX0(xList):
            assert len(xList) >= 1
            expPow = xList[0] * 0.2
            sinSum = reduce(lambda x1, x2: x1 + x2, xList) - xList[0]
            return [ exp(-abs(expPow)) * sin(sinSum) ]
        return attenSinX0
    elif funcName == "AttenSin":
        """
        no abs(), phase of sin and exponent are all controlled by sum(xList)
        suitable for plotting: y - sum(xList)
        """
        def attenSin(xList):
            sinSum = sum(xList)
            return [ exp(-0.06 * sinSum) * sin(sinSum) ]
        return attenSin
    elif funcName == "ANN-bp":
        """
        output is generated by the ANN,
        and the data is intended to be learned by ANN in return
        """
        def forwardANN(xList, struct, activ_list, cost_type):
            net = Net_structure(struct, [activation_dict[n] for n in activ_list], cost_dict[cost_type])
            w_list = []
            b_list = []
            for l in range(len(struct) - 1):
                w_list += [(array(range(struct[l]*struct[l+1])).reshape(struct[l], struct[l+1]) + float(l)) / 100.]
                b_list += [(array(range(struct[l+1])) - float(l)) / 100.]

            net.set_w_b(w_list, b_list)
            return list(net.net_act_forward(array(xList)))
        return forwardANN
